{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc64b819504dc85b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Install Necessary Libraries [Check README > System Requirements]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15b46d",
   "metadata": {},
   "source": [
    "### Run this command only if you don't want to use \"Virtual Environment\" and want to use your own \"Local System\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e0096d7b01cf1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %pip install pandas torch torchaudio librosa transformers==4.41.2 ipywidgets datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852befd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e68fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c8a68ef932d25",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c98bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbb7d1f86f1df0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118bf326726a0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the current working directory (where the Jupyter notebook is located)\n",
    "project_folder = os.path.abspath(os.getcwd())\n",
    "print(\"Project Folder : \", project_folder)\n",
    "\n",
    "# Construct the relative path to the validated.tsv file\n",
    "tsv_file_path = os.path.join(project_folder, \"Data\", \"validated.tsv\")\n",
    "\n",
    "print(\"TSV Data Path : \", tsv_file_path)\n",
    "\n",
    "# Load TSV files\n",
    "validated_df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "# Process audio files\n",
    "def load_audio(file_path):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "# Example usage with a relative path\n",
    "audio_file_path = os.path.join(project_folder, \"Data\", \"clips\", \"common_voice_en_34925860.mp3\")\n",
    "waveform, sample_rate = load_audio(audio_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c433b799510f4b90",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Balance the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135da7c1b3be01e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "selected_accents = [\"Canadian English\", \"England English\"]\n",
    "filtered_df = validated_df[validated_df['accents'].isin(selected_accents)]\n",
    "\n",
    "balanced_data = pd.DataFrame()\n",
    "\n",
    "for accent in selected_accents:\n",
    "    accent_data = filtered_df[filtered_df['accents'] == accent]\n",
    "    sampled_data = resample(accent_data, n_samples=300, random_state=42)\n",
    "    balanced_data = pd.concat([balanced_data, sampled_data])\n",
    "\n",
    "# Print the class distribution in the balanced dataset\n",
    "print(\"Balanced Class Distribution:\\n\", balanced_data['accents'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a0a9f48c9662a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a96c40027e7544",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Understand the distribution of classes\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "class_column = 'accents'\n",
    "class_distribution = balanced_data[class_column].value_counts()\n",
    "print(\"Class Distribution:\\n\", class_distribution)\n",
    "\n",
    "# List of specific classes to focus on\n",
    "selected_classes = [\"Canadian English\", \"England English\"]\n",
    "\n",
    "# Listen to some audio samples for specific classes\n",
    "for label in selected_classes:\n",
    "    class_df = balanced_data[balanced_data[class_column] == label]\n",
    "\n",
    "    if not class_df.empty:\n",
    "        sample = class_df.iloc[0]\n",
    "        audio_path = os.path.join(project_folder, \"Data\", \"clips\", sample['path'])\n",
    "\n",
    "        # Load actual audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        print(f\"\\nPlaying audio for class: {label}\")\n",
    "        display(Audio(waveform.numpy(), rate=sr))\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\nNo samples found for class: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0e0627f6f47ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c2a3a5aca6191",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove rows with NaN values\n",
    "balanced_data = balanced_data.dropna(subset=[class_column])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    balanced_data, \n",
    "    test_size=0.2, \n",
    "    stratify=balanced_data[class_column],  # Preserve class distribution\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d246f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename split datasets for clarity\n",
    "\n",
    "train_df = train_df.copy()\n",
    "\n",
    "valid_df = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bebeda5e97057",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model Selection code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate DEVELOPER Mode in System Settings to RUN this MODEL\n",
    "# RUN this code to support symlinks by huggingface\n",
    "\n",
    "%pip install huggingface_hub[hf_xet] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8882e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "print(\"Dataset Sampling Rate (in Hz) : \", extractor.sampling_rate)  # Usually 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdcb9718c816a5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Wav2Vec2 Processor\n",
    "\n",
    "sampling_rate = 16000\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\", \n",
    "    feature_extractor_kwargs={\"sampling_rate\": sampling_rate}\n",
    "    )\n",
    "\n",
    "# Encode Accent Labels\n",
    "\n",
    "selected_accents = ['Canadian English', 'England English']\n",
    "label2id = {label: idx for idx, label in enumerate(selected_accents)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Class Distribution:\", len(selected_accents))\n",
    "print(\"Unique Accents:\", train_df['accents'].nunique())\n",
    "print(\"Selected Classes:\", selected_classes)\n",
    "\n",
    "validated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode in DataFrame\n",
    "\n",
    "train_df[\"label\"] = train_df[\"accents\"].map(label2id)\n",
    "valid_df[\"label\"] = valid_df[\"accents\"].map(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4cce4ce9cd15f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Checking Data Shapes and Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65504e7d6cd0b7be",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "balanced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ed84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total missing values per column\n",
    "\n",
    "print(balanced_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary 'segment' column\n",
    "\n",
    "balanced_data = balanced_data.drop(columns=['segment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37993e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have missing values in gender or age only if needed\n",
    "\n",
    "balanced_data = balanced_data.dropna(subset=['gender', 'age']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e027797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows that contain any NaN values\n",
    "\n",
    "missing_rows = balanced_data[balanced_data.isna().any(axis=1)]\n",
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d26fa83536c0b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63a24c399d9e1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "balanced_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108d720642f70a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "balanced_data.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2a22b55226aa5d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if we are dealing with an imbalanced dataset\n",
    "balanced_data['locale'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92918e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Encode Labels + Load Processor\n",
    "\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "selected_accents = ['Canadian English', 'England English']\n",
    "label2id = {label: idx for idx, label in enumerate(selected_accents)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "balanced_data[\"label\"] = balanced_data[\"accents\"].map(label2id)\n",
    "\n",
    "sampling_rate = 16000\n",
    "processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\", \n",
    "    feature_extractor_kwargs={\"sampling_rate\": sampling_rate}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c40ed",
   "metadata": {},
   "source": [
    "# Train - Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45dd6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    balanced_data,\n",
    "    test_size=0.2,\n",
    "    stratify=balanced_data[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac9a4d",
   "metadata": {},
   "source": [
    "# Check Dataset Audio Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Step 1: Set project folder path\n",
    "project_folder = os.path.abspath(os.getcwd())\n",
    "\n",
    "# Step 2: Function to get full audio path\n",
    "def get_full_audio_path(relative_path):\n",
    "    return os.path.join(project_folder, \"Data\", \"clips\", relative_path)\n",
    "\n",
    "# Step 3: Filter only selected accent classes\n",
    "selected_classes = [\"Canadian English\", \"England English\"]\n",
    "balanced_data = validated_df[validated_df['accents'].isin(selected_classes)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Step 4: Function to get duration in seconds\n",
    "def get_audio_duration(file_path):\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        duration = waveform.shape[1] / sample_rate\n",
    "        return round(duration, 2)  # round to 2 decimal places\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 5: Apply to all rows\n",
    "durations = []\n",
    "for idx, row in tqdm(balanced_data.iterrows(), total=len(balanced_data)):\n",
    "    path = get_full_audio_path(row['path'])\n",
    "    duration = get_audio_duration(path)\n",
    "    durations.append(duration)\n",
    "\n",
    "# Step 6: Add durations to DataFrame\n",
    "balanced_data['duration_sec'] = durations\n",
    "\n",
    "# Step 7: Basic stats and check\n",
    "print(\"\\nðŸ“Š Audio Duration Summary:\")\n",
    "print(balanced_data['duration_sec'].describe())\n",
    "\n",
    "print(\"\\nðŸŽ§ Audio files shorter than 5 seconds:\")\n",
    "print(balanced_data[balanced_data['duration_sec'] < 5][['path', 'duration_sec']])\n",
    "\n",
    "print(\"\\nðŸŽ§ Audio files longer than 5 seconds:\")\n",
    "print(balanced_data[balanced_data['duration_sec'] > 5][['path', 'duration_sec']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b37a09",
   "metadata": {},
   "source": [
    "# Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7700985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class AccentDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, label2id, audio_base_path):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.label2id = label2id\n",
    "        self.audio_base_path = audio_base_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = os.path.join(self.audio_base_path, row['path'])\n",
    "\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        input_values = self.processor(\n",
    "            waveform.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=16000 * 4      # e.g., max 4 seconds audio at 16kHz\n",
    "        ).input_values.squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": input_values,\n",
    "            \"label\": row[\"label\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0c4dd",
   "metadata": {},
   "source": [
    "# Define Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c74276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classification Model\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model\n",
    "\n",
    "# Define Classification Model\n",
    "class Wav2Vec2Classifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(Wav2Vec2Classifier, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(self.wav2vec2.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        pooled_output = hidden_states.mean(dim=1)  # Mean pooling\n",
    "        logits = self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a502cc35",
   "metadata": {},
   "source": [
    "# Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data Loaders\n",
    "\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "def convert_to_dict(dataset):\n",
    "    input_values = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        input_values.append(item['input_values'].numpy())\n",
    "        labels.append(item['label'])\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"label\": labels\n",
    "    }\n",
    "\n",
    "audio_base_path = os.path.join(os.getcwd(), \"Data\", \"clips\")\n",
    "\n",
    "train_dataset = AccentDataset(train_df, processor, label2id, audio_base_path)\n",
    "test_dataset = AccentDataset(test_df, processor, label2id, audio_base_path)\n",
    "\n",
    "train_dict = convert_to_dict(train_dataset)\n",
    "test_dict = convert_to_dict(test_dataset)\n",
    "\n",
    "hf_train = HFDataset.from_dict(train_dict)\n",
    "hf_test = HFDataset.from_dict(test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f49e6",
   "metadata": {},
   "source": [
    "# Run this Command in \"Terminal - CMD\" inside \"VS Code\" to use GPU :\n",
    "\n",
    "### pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version : \", torch.version.cuda)\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836883d6",
   "metadata": {},
   "source": [
    "# Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce6b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train_dataset & eval_dataset\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 80% train, 20% eval split\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "eval_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, eval_dataset = random_split(train_dataset, [train_size, eval_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd695dcb",
   "metadata": {},
   "source": [
    "# Define Training Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Arguments \n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2d79e",
   "metadata": {},
   "source": [
    "# Define Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c40b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Compute Metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions = pred.predictions.argmax(-1)\n",
    "    labels = pred.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03994902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "\n",
    "model = Wav2Vec2Classifier(num_labels=2)\n",
    "\n",
    "# Now call the Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,  # Optional: only needed if using processor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010d86b",
   "metadata": {},
   "source": [
    "# Train & Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c70cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Evaluate the Model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72cce1",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4bc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "\n",
    "trainer.save_model(\"Model\")\n",
    "processor.save_pretrained(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb105f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43ef2a",
   "metadata": {},
   "source": [
    "# Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f78cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create the outputs directory if it doesn't exist\n",
    "os.makedirs(\"Output\", exist_ok=True)\n",
    "\n",
    "preds = trainer.predict(eval_dataset)\n",
    "predicted_labels = preds.predictions.argmax(axis=1)\n",
    "true_labels = preds.label_ids\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"True Label\": true_labels,\n",
    "    \"Predicted Label\": predicted_labels\n",
    "})\n",
    "df.to_csv(\"Output/eval_predictions.csv\", index=False)\n",
    "\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f67b1",
   "metadata": {},
   "source": [
    "# Accuracy, Precision, Recall & F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Run prediction\n",
    "preds_output = trainer.predict(eval_dataset)\n",
    "y_pred = preds_output.predictions.argmax(axis=1)\n",
    "y_true = preds_output.label_ids\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Print\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall: .4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b2ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create \"Graphs\" folder if it doesn't exist\n",
    "os.makedirs(\"Graphs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define metrics\n",
    "metrics = {\n",
    "    \"Accuracy\": round(accuracy * 100, 2),\n",
    "    \"Precision\": round(precision * 100, 2),\n",
    "    \"Recall\": round(recall * 100, 2),\n",
    "    \"F1 Score\": round(f1 * 100, 2),\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(metrics, index=[\"Score (%)\"]).T\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"Output/model_metrics.csv\")       # For CSV\n",
    "\n",
    "# Plot clean bar graph\n",
    "plt.figure(figsize=(6, 4))\n",
    "bars = plt.bar(df.index, df[\"Score (%)\"], color='skyblue')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, height + 1, f'{height}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.ylim(0, 110)\n",
    "plt.ylabel(\"Score (%)\")\n",
    "plt.title(\"Model Evaluation Metrics\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Final_Metrics_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2975ba",
   "metadata": {},
   "source": [
    "# Plotting GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Data\n",
    "metrics = {\n",
    "    \"Accuracy\": 97.83,\n",
    "    \"Precision\": 100.00,\n",
    "    \"Recall\": 95.35,\n",
    "    \"F1 Score\": 97.62\n",
    "}\n",
    "\n",
    "# Bar Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette=\"Blues_d\")\n",
    "\n",
    "plt.title(\"Model Performance Metrics\", fontsize=16)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=12)\n",
    "plt.ylim(0, 110)\n",
    "\n",
    "for i, v in enumerate(metrics.values()):\n",
    "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Bar_Graph_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart Graph\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(metrics.values(), labels=metrics.keys(), autopct='%1.1f%%', colors=sns.color_palette(\"pastel\"))\n",
    "plt.title(\"Model Metric Distribution\")\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Pie_Chart_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ddd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar (Spider) Chart\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Labels and values\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "values = [97.83, 100.00, 95.35, 97.62]\n",
    "values += values[:1]  # Repeat first value to close the circle\n",
    "\n",
    "# Angle for each axis\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Radar plot\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "ax.plot(angles, values, color='blue', linewidth=2)\n",
    "ax.fill(angles, values, color='skyblue', alpha=0.4)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticks([20, 40, 60, 80, 100])\n",
    "ax.set_title(\"Model Performance Radar Chart\", y=1.1)\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Spider_Chart_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal Bar Plot\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": 97.83,\n",
    "    \"Precision\": 100.00,\n",
    "    \"Recall\": 95.35,\n",
    "    \"F1 Score\": 97.62\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.barplot(y=list(metrics.keys()), x=list(metrics.values()), palette='magma')\n",
    "plt.xlabel(\"Percentage (%)\")\n",
    "plt.title(\"Model Evaluation Metrics\")\n",
    "for i, (k, v) in enumerate(metrics.items()):\n",
    "    plt.text(v + 0.5, i, f\"{v:.2f}%\", va='center')\n",
    "plt.xlim(0, 110)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Horizontal_Bar_chart_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plot (Comparison Curve)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(list(metrics.keys()), list(metrics.values()), marker='o', linestyle='-', color='green')\n",
    "plt.title(\"Model Metrics Comparison\")\n",
    "plt.ylabel(\"Percentage (%)\")\n",
    "plt.ylim(90, 105)\n",
    "for i, (k, v) in enumerate(metrics.items()):\n",
    "    plt.text(i, v + 0.5, f\"{v:.2f}%\", ha='center')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Line_chart_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd778fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Canadian\", \"England\"], yticklabels=[\"Canadian\", \"England\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Confusion_Matrix_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa23fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ROC Curve\n",
    "\n",
    "y_probs = preds_output.predictions[:, 1]  # Probability for positive class (class 1)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/ROC_curve_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Precision-Recall Curve\n",
    "\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_probs)\n",
    "plt.figure()\n",
    "plt.plot(recall_vals, precision_vals, color=\"purple\", lw=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Precision_Recall_curve_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "loss_values = [entry['loss'] for entry in log_history if 'loss' in entry]\n",
    "plt.plot(loss_values)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "\n",
    "# Save the plot\n",
    "timestamp = time.strftime(\"%d%m%Y-%H%M%S\")\n",
    "plt.savefig(f\"Graphs/Training_Loss_curve_{timestamp}.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccdd192d3b12d67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6cc174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e403512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
